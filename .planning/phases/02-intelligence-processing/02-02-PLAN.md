---
phase: 02-intelligence-processing
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - backend/llm/narrative.py
  - backend/llm/extraction.py
  - backend/processors/batch_articles.py
autonomous: true
user_setup:
  - service: anthropic
    why: "Claude API for narrative coordination detection and entity extraction"
    env_vars:
      - name: ANTHROPIC_API_KEY
        source: "Anthropic Console -> API Keys (console.anthropic.com)"
  - service: supabase
    why: "Read articles, write narrative_events"
    env_vars:
      - name: SUPABASE_URL
        source: "Supabase Dashboard -> Settings -> API -> Project URL"
      - name: SUPABASE_KEY
        source: "Supabase Dashboard -> Settings -> API -> anon/public key"

must_haves:
  truths:
    - "Article batches from Supabase are analyzed for synchronized phrasing and coordination scores (0-100) are written to narrative_events table"
    - "Entity extraction pulls military units, equipment types, locations with lat/lon, and timestamps from article text with source spans"
    - "Prompt uses XML tags to separate instructions from untrusted article data for injection defense"
    - "Batch processing respects rate limits via semaphore and retries transient errors"
  artifacts:
    - path: "backend/llm/narrative.py"
      provides: "detect_narrative_coordination() — analyzes article batch, returns NarrativeCoordination"
      contains: "async def detect_narrative_coordination"
    - path: "backend/llm/extraction.py"
      provides: "extract_entities() — extracts structured entities from text, returns EntityExtraction"
      contains: "async def extract_entities"
    - path: "backend/processors/batch_articles.py"
      provides: "process_article_batch() — reads articles from Supabase, runs narrative detection + entity extraction, writes results"
      contains: "async def process_article_batch"
  key_links:
    - from: "backend/llm/narrative.py"
      to: "backend/llm/schemas.py"
      via: "NarrativeCoordination schema for structured output"
      pattern: "from backend\\.llm\\.schemas import NarrativeCoordination"
    - from: "backend/llm/narrative.py"
      to: "backend/llm/clients.py"
      via: "AsyncAnthropic client"
      pattern: "from backend\\.llm\\.clients import get_anthropic_client"
    - from: "backend/processors/batch_articles.py"
      to: "backend/llm/narrative.py"
      via: "Calls detect_narrative_coordination with article data"
      pattern: "detect_narrative_coordination"
    - from: "backend/processors/batch_articles.py"
      to: "backend/llm/extraction.py"
      via: "Calls extract_entities for each article"
      pattern: "extract_entities"
---

<objective>
Implement the narrative coordination detector (LLM-01) and entity extraction (LLM-03) with a batch article processing pipeline that reads from Supabase and writes structured intelligence to narrative_events.

Purpose: This is the first of two intelligence streams (narrative). It detects when state media outlets simultaneously adopt synchronized phrasing — the key signal that a centralised directive has been issued. Entity extraction enriches the output with structured military entities.

Output: `backend/llm/narrative.py` (LLM-01), `backend/llm/extraction.py` (LLM-03), `backend/processors/batch_articles.py` (pipeline).
</objective>

<execution_context>
@/Users/gremmy/.claude/get-shit-done/workflows/execute-plan.md
@/Users/gremmy/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-intelligence-processing/02-RESEARCH.md
@.planning/phases/02-intelligence-processing/02-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement narrative coordination detector and entity extraction</name>
  <files>
    backend/llm/narrative.py
    backend/llm/extraction.py
  </files>
  <action>
    1. Create `backend/llm/narrative.py` with `async def detect_narrative_coordination(articles: List[dict]) -> NarrativeCoordination`:
       - Accept list of dicts with keys: outlet, title, content, published_at
       - Truncate each article content to NARRATIVE_ARTICLE_SNIPPET_LEN chars using utils.truncate_text
       - Build prompt with XML tags separating instructions from data:
         ```
         <instructions>
         Analyze these state media articles for narrative coordination.
         Look for:
         1. Identical or near-identical phrases (3+ words) repeated across multiple outlets
         2. Same geographic focus across outlets (e.g., Taiwan Strait, South China Sea)
         3. Synchronized themes suggesting centralised directive

         Score coordination 0-100:
         - 0-30: Independent reporting (different topics, natural language variation)
         - 30-70: Moderate coordination (shared themes, some phrasing overlap)
         - 70-100: Strong coordination (synchronized phrasing across 3+ outlets, identical narratives)

         Extract all synchronized phrases (exact 3+ word sequences appearing in 2+ outlets).
         Identify the primary geographic region mentioned across outlets.
         List narrative themes (1-3 words each).
         Rate your confidence in the assessment (0-100).
         </instructions>

         <articles>
         {formatted article data with outlet, title, snippet}
         </articles>
         ```
       - Use get_anthropic_client() and call client.messages.parse() with output_format=NarrativeCoordination
       - Apply retry decorator from utils
       - Log call with structlog (model, tokens, cost, duration)
       - Cap batch to NARRATIVE_MAX_ARTICLES articles

    2. Create `backend/llm/extraction.py` with `async def extract_entities(text: str, source_type: str = "article") -> EntityExtraction`:
       - Accept raw text and source type identifier
       - Truncate text to max_article_tokens or max_post_tokens based on source_type
       - Build prompt with XML tags:
         ```
         <instructions>
         Extract military-relevant entities from this text.

         Entity types:
         - military_unit: Unit names, designations (e.g., "PLA Eastern Theater Command", "Type 052D")
         - equipment: Weapon systems, vehicles, platforms (e.g., "J-20 fighter", "DF-21D")
         - location: Geographic locations with coordinates if available (e.g., "Taiwan Strait", "24.5N 118.2E")
         - timestamp: Dates, times, temporal references (e.g., "Tuesday morning", "March 15")

         Rules:
         1. Extract ONLY entities explicitly present in text — do NOT infer or fabricate
         2. Include exact source_span (verbatim text snippet containing the entity)
         3. For locations, include latitude/longitude if coordinates present in text, null otherwise
         4. Rate confidence 0-100 for each entity
         5. Return empty entities list if no military-relevant entities found
         </instructions>

         <data>
         {truncated text}
         </data>
         ```
       - Use get_anthropic_client() with output_format=EntityExtraction
       - Apply retry decorator
       - Log call with structlog
  </action>
  <verify>
    - `python -c "from backend.llm.narrative import detect_narrative_coordination; print('Narrative OK')"` succeeds
    - `python -c "from backend.llm.extraction import extract_entities; print('Extraction OK')"` succeeds
    - Both functions are async (inspect.iscoroutinefunction returns True)
    - Prompts use XML tags (`<instructions>`, `<articles>`, `<data>`) for injection defense
  </verify>
  <done>
    Narrative coordination detector accepts article batches and returns NarrativeCoordination with coordination scores. Entity extractor accepts text and returns EntityExtraction with structured entities. Both use retry logic, structured logging, and XML-tagged prompts.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement batch article processing pipeline</name>
  <files>
    backend/processors/batch_articles.py
  </files>
  <action>
    Create `backend/processors/batch_articles.py` with:

    1. `async def fetch_unprocessed_articles(supabase_client, limit: int = 20) -> List[dict]`:
       - Query articles table for rows where `processed_at IS NULL`
       - Order by `published_at DESC`
       - Return list of article dicts
       - Use `asyncio.to_thread()` to wrap synchronous Supabase client call (per research: Supabase Python client uses sync HTTP)

    2. `async def write_narrative_event(supabase_client, coordination: NarrativeCoordination, article_ids: List[str]) -> dict`:
       - Insert into narrative_events table with fields:
         - coordination_score, synchronized_phrases, outlet_count, geographic_focus, themes, confidence
         - article_ids (array of source article UUIDs)
         - created_at (UTC timestamp)
       - Use `asyncio.to_thread()` for Supabase write

    3. `async def write_entities(supabase_client, entities: EntityExtraction, source_id: str, source_type: str) -> None`:
       - Bulk insert extracted entities into an entities enrichment field or separate table
       - Include source_id and source_type for traceability

    4. `async def process_article_batch() -> dict`:
       - Main pipeline orchestrator:
         a. Fetch unprocessed articles
         b. If no articles, return early with status
         c. Run detect_narrative_coordination on the batch
         d. Run extract_entities on each article concurrently (using semaphore from settings)
         e. Write narrative_event to Supabase
         f. Write entities to Supabase
         g. Mark articles as processed (set processed_at timestamp)
         h. Return summary dict with articles_processed, coordination_score, entities_extracted counts
       - Wrap entire pipeline in try/except with structured logging
       - Log pipeline start, completion, and any errors

    5. `if __name__ == "__main__":` block for standalone execution:
       ```python
       import asyncio
       asyncio.run(process_article_batch())
       ```
  </action>
  <verify>
    - `python -c "from backend.processors.batch_articles import process_article_batch; print('Pipeline OK')"` succeeds
    - process_article_batch is async (inspect.iscoroutinefunction returns True)
    - Code uses asyncio.to_thread() for Supabase calls (not blocking async loop)
    - Pipeline handles empty article list gracefully (returns early, no errors)
  </verify>
  <done>
    Batch article processor reads unprocessed articles from Supabase, runs narrative coordination detection + entity extraction via Claude, writes results to narrative_events table, and marks articles as processed. Pipeline is fully async with non-blocking Supabase writes.
  </done>
</task>

</tasks>

<verification>
- `python -c "from backend.llm.narrative import detect_narrative_coordination; from backend.llm.extraction import extract_entities; from backend.processors.batch_articles import process_article_batch; print('Phase 2 Plan 02 OK')"` succeeds
- Narrative prompt includes XML tags for injection defense
- Entity extraction prompt includes grounding instruction ("ONLY entities present in text")
- Batch processor uses asyncio.to_thread() for all Supabase operations
- All async functions use retry decorators from utils
</verification>

<success_criteria>
- detect_narrative_coordination() accepts article batch, returns NarrativeCoordination with score 0-100
- extract_entities() accepts text, returns EntityExtraction with typed entities and source spans
- process_article_batch() orchestrates full pipeline: fetch -> analyze -> extract -> write -> mark processed
- All LLM calls use structured outputs, retry logic, rate limiting, and cost logging
</success_criteria>

<output>
After completion, create `.planning/phases/02-intelligence-processing/02-02-SUMMARY.md`
</output>

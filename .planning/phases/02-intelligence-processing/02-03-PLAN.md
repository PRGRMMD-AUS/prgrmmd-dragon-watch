---
phase: 02-intelligence-processing
plan: 03
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - backend/llm/classifier.py
  - backend/processors/batch_posts.py
autonomous: true
user_setup:
  - service: openai
    why: "GPT-4o Mini for cost-effective bulk classification of civilian posts"
    env_vars:
      - name: OPENAI_API_KEY
        source: "OpenAI Platform -> API Keys (platform.openai.com/api-keys)"

must_haves:
  truths:
    - "Social posts from Supabase are classified into military-relevant categories (convoy, naval, flight, restricted_zone, not_relevant) with confidence scores"
    - "Classification results with location and confidence are written to movement_events table"
    - "GPT-4o Mini is used for classification (cost optimization — 16x cheaper than Claude)"
    - "Batch processing classifies multiple posts concurrently with semaphore-based rate limiting"
  artifacts:
    - path: "backend/llm/classifier.py"
      provides: "classify_civilian_post() — classifies single post, returns PostClassification"
      contains: "async def classify_civilian_post"
    - path: "backend/processors/batch_posts.py"
      provides: "process_post_batch() — reads posts from Supabase, classifies, writes movement_events"
      contains: "async def process_post_batch"
  key_links:
    - from: "backend/llm/classifier.py"
      to: "backend/llm/schemas.py"
      via: "PostClassification schema for structured output"
      pattern: "from backend\\.llm\\.schemas import PostClassification"
    - from: "backend/llm/classifier.py"
      to: "backend/llm/clients.py"
      via: "AsyncOpenAI client for GPT-4o Mini"
      pattern: "from backend\\.llm\\.clients import get_openai_client"
    - from: "backend/processors/batch_posts.py"
      to: "backend/llm/classifier.py"
      via: "Calls classify_civilian_post for each post"
      pattern: "classify_civilian_post"
    - from: "backend/processors/batch_posts.py"
      to: "backend/llm/extraction.py"
      via: "Calls extract_entities for military-relevant posts"
      pattern: "extract_entities"
---

<objective>
Implement the civilian post classifier (LLM-02) with a batch post processing pipeline that reads social posts from Supabase, classifies military relevance using GPT-4o Mini, enriches with entity extraction, and writes movement_events.

Purpose: This is the second intelligence stream (movement). Civilian social media posts revealing military activity (convoys, naval sightings, flight activity, restricted zones) are the ground-truth signal that complements narrative coordination detection.

Output: `backend/llm/classifier.py` (LLM-02), `backend/processors/batch_posts.py` (pipeline).
</objective>

<execution_context>
@/Users/gremmy/.claude/get-shit-done/workflows/execute-plan.md
@/Users/gremmy/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-intelligence-processing/02-RESEARCH.md
@.planning/phases/02-intelligence-processing/02-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement civilian post classifier</name>
  <files>
    backend/llm/classifier.py
  </files>
  <action>
    Create `backend/llm/classifier.py` with:

    1. `async def classify_civilian_post(post_text: str) -> PostClassification`:
       - Truncate post_text to max_post_tokens using utils.truncate_text
       - Build prompt with XML tags:
         ```
         <instructions>
         Classify this social media post for military-relevant movement indicators.

         Categories:
         - convoy: Road convoy, military vehicles in formation, troop transport
         - naval: Ship/submarine activity, port movements, naval exercises
         - flight: Military aircraft, unusual flight patterns, airspace restrictions
         - restricted_zone: Access restrictions, roadblocks, evacuations, security cordons
         - not_relevant: Civilian activity, no military indicators

         Extract:
         - Category (exactly one from above)
         - Location mentioned (city, region, or coordinates if present; null if absent)
         - Confidence 0-100 (how certain is this classification)
         - Brief reasoning (1-2 sentences why this category)

         Indicators to look for: vehicle types, movement patterns, uniform/equipment mentions,
         restricted access, unusual military presence, security checkpoints, naval vessel sightings.
         </instructions>

         <post>
         {truncated post text}
         </post>
         ```
       - Use get_openai_client() and call client.beta.chat.completions.parse() with response_format=PostClassification
       - NOTE on OpenAI structured outputs: Use `client.beta.chat.completions.parse()` method which accepts Pydantic models directly via `response_format=PostClassification`. This is the current stable approach. Validate response with `response.choices[0].message.parsed`.
       - Apply retry decorator from utils (configured for OpenAI exceptions: openai.RateLimitError, openai.APITimeoutError, openai.InternalServerError)
       - Log call with structlog (model="gpt-4o-mini", tokens, cost, duration)

    2. `async def classify_posts_batch(posts: List[dict], max_concurrent: int = None) -> List[PostClassification]`:
       - Accept list of post dicts with key: content
       - Use semaphore from settings (max_concurrent_openai) for rate limiting
       - Process all posts concurrently with asyncio.gather
       - Return list of PostClassification results
       - Handle individual post failures gracefully (log error, skip post, continue batch)
  </action>
  <verify>
    - `python -c "from backend.llm.classifier import classify_civilian_post, classify_posts_batch; print('Classifier OK')"` succeeds
    - Both functions are async
    - classify_posts_batch uses asyncio.Semaphore for rate limiting
    - Prompt uses XML tags for injection defense
  </verify>
  <done>
    Civilian post classifier accepts post text, returns PostClassification with category, location, confidence, and reasoning. Batch function processes multiple posts concurrently with semaphore-based rate limiting. Uses GPT-4o Mini for cost optimization.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement batch post processing pipeline</name>
  <files>
    backend/processors/batch_posts.py
  </files>
  <action>
    Create `backend/processors/batch_posts.py` with:

    1. `async def fetch_unprocessed_posts(supabase_client, limit: int = 50) -> List[dict]`:
       - Query social_posts table for rows where `processed_at IS NULL`
       - Order by `created_at DESC`
       - Return list of post dicts
       - Use `asyncio.to_thread()` for Supabase sync client

    2. `async def write_movement_event(supabase_client, classification: PostClassification, post_id: str, entities: EntityExtraction = None) -> dict`:
       - Only write if category != not_relevant (skip irrelevant posts)
       - Insert into movement_events table with fields:
         - category (convoy/naval/flight/restricted_zone)
         - location
         - confidence
         - reasoning
         - post_id (source post UUID)
         - entities (extracted entity data if available)
         - created_at (UTC timestamp)
       - Use `asyncio.to_thread()` for Supabase write

    3. `async def process_post_batch() -> dict`:
       - Main pipeline orchestrator:
         a. Fetch unprocessed posts
         b. If no posts, return early with status
         c. Run classify_posts_batch on all posts concurrently
         d. For posts classified as military-relevant (not "not_relevant"):
            - Run extract_entities (from backend.llm.extraction) on each relevant post concurrently
         e. Write movement_events to Supabase for relevant posts
         f. Mark all posts as processed (set processed_at timestamp)
         g. Return summary dict: posts_processed, relevant_count, categories breakdown, entities_extracted
       - Wrap in try/except with structured logging
       - Log pipeline start, per-category counts, completion

    4. `if __name__ == "__main__":` block:
       ```python
       import asyncio
       asyncio.run(process_post_batch())
       ```
  </action>
  <verify>
    - `python -c "from backend.processors.batch_posts import process_post_batch; print('Post Pipeline OK')"` succeeds
    - process_post_batch is async
    - Pipeline filters not_relevant posts before entity extraction (cost optimization)
    - Pipeline handles empty post list gracefully
    - Uses asyncio.to_thread() for Supabase calls
  </verify>
  <done>
    Batch post processor reads unprocessed posts from Supabase, classifies with GPT-4o Mini, runs entity extraction on relevant posts with Claude, writes movement_events for military-relevant posts, and marks all posts as processed. Irrelevant posts are filtered before entity extraction to save API costs.
  </done>
</task>

</tasks>

<verification>
- `python -c "from backend.llm.classifier import classify_civilian_post, classify_posts_batch; from backend.processors.batch_posts import process_post_batch; print('Phase 2 Plan 03 OK')"` succeeds
- Classifier uses GPT-4o Mini (not Claude) for cost optimization
- Batch processor filters not_relevant posts before entity extraction
- All Supabase operations use asyncio.to_thread()
- Both LLM calls use structured outputs and retry logic
</verification>

<success_criteria>
- classify_civilian_post() accepts post text, returns PostClassification with category enum
- classify_posts_batch() processes posts concurrently with semaphore rate limiting
- process_post_batch() orchestrates: fetch -> classify -> filter -> extract entities -> write movement_events
- Not-relevant posts are filtered before entity extraction (cost optimization)
- Uses GPT-4o Mini for classification, Claude for entity extraction on relevant posts only
</success_criteria>

<output>
After completion, create `.planning/phases/02-intelligence-processing/02-03-SUMMARY.md`
</output>

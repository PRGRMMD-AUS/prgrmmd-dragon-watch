---
phase: 02-intelligence-processing
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/llm/__init__.py
  - backend/llm/config.py
  - backend/llm/schemas.py
  - backend/llm/clients.py
  - backend/llm/utils.py
  - backend/processors/__init__.py
  - backend/config/__init__.py
  - backend/config/settings.py
  - backend/config/.env.example
  - requirements.txt
autonomous: true

must_haves:
  truths:
    - "LLM client initialization works for both Anthropic (Claude) and OpenAI (GPT-4o Mini) with async support"
    - "All Pydantic output schemas validate correctly with field constraints (score ranges 0-100, enums for categories/threat levels)"
    - "Retry logic with exponential backoff handles RateLimitError, APITimeoutError, InternalServerError"
    - "Structured logging captures model, duration, token usage, and estimated cost for every LLM call"
    - "Environment config loads API keys from .env with validation errors on missing keys"
  artifacts:
    - path: "backend/llm/config.py"
      provides: "LLM provider configuration (model names, token limits, rate limits)"
    - path: "backend/llm/schemas.py"
      provides: "All Pydantic models: NarrativeCoordination, PostClassification, EntityExtraction, IntelligenceBrief"
    - path: "backend/llm/clients.py"
      provides: "AsyncAnthropic and AsyncOpenAI client factory functions"
    - path: "backend/llm/utils.py"
      provides: "Retry decorators, semaphore rate limiter, structured logging wrapper"
    - path: "backend/config/settings.py"
      provides: "Pydantic Settings with ANTHROPIC_API_KEY, OPENAI_API_KEY, SUPABASE_URL, SUPABASE_KEY"
    - path: "requirements.txt"
      provides: "All Phase 2 Python dependencies"
  key_links:
    - from: "backend/llm/clients.py"
      to: "backend/config/settings.py"
      via: "Settings import for API keys"
      pattern: "from backend\\.config\\.settings import"
    - from: "backend/llm/utils.py"
      to: "tenacity"
      via: "Retry decorator using tenacity"
      pattern: "from tenacity import"
    - from: "backend/llm/utils.py"
      to: "structlog"
      via: "Structured logging"
      pattern: "structlog\\.get_logger"
---

<objective>
Create the shared foundation layer for all LLM processing: configuration management, Pydantic output schemas, async client factories, and utility functions (retry logic, rate limiting, structured logging).

Purpose: Every subsequent plan in Phase 2 depends on these shared components. Establishing them first ensures consistent patterns across all four LLM requirements (LLM-01 through LLM-04).

Output: `backend/llm/` module with config, schemas, clients, utils. `backend/config/` with pydantic-settings. `requirements.txt` with all Phase 2 dependencies.
</objective>

<execution_context>
@/Users/gremmy/.claude/get-shit-done/workflows/execute-plan.md
@/Users/gremmy/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-intelligence-processing/02-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create project structure, dependencies, and configuration</name>
  <files>
    requirements.txt
    backend/__init__.py
    backend/config/__init__.py
    backend/config/settings.py
    backend/config/.env.example
    backend/llm/__init__.py
    backend/processors/__init__.py
  </files>
  <action>
    1. Create `requirements.txt` with ALL Phase 2 dependencies (append to existing if present):
       ```
       # Phase 1 dependencies (preserve existing)
       # Phase 2: LLM Processing
       anthropic>=0.39.0,<1.0
       openai>=1.54.0,<2.0
       pydantic>=2.0,<3.0
       pydantic-settings>=2.0,<3.0
       httpx>=0.27.0,<1.0
       tenacity>=8.0,<9.0
       structlog>=24.0
       ```

    2. Create `backend/__init__.py`, `backend/config/__init__.py`, `backend/llm/__init__.py`, `backend/processors/__init__.py` as empty init files.

    3. Create `backend/config/settings.py` using pydantic-settings:
       - Class `Settings(BaseSettings)` with fields:
         - `anthropic_api_key: str` (required)
         - `openai_api_key: str` (required)
         - `supabase_url: str` (required)
         - `supabase_key: str` (required)
         - `claude_model: str = "claude-sonnet-4-5"` (default model for narrative/extraction/briefs)
         - `openai_model: str = "gpt-4o-mini"` (default model for classification)
         - `max_concurrent_claude: int = 5` (semaphore limit for Claude)
         - `max_concurrent_openai: int = 20` (semaphore limit for GPT-4o Mini)
         - `max_article_tokens: int = 1000` (truncation limit for articles)
         - `max_post_tokens: int = 200` (truncation limit for posts)
       - Use `model_config = SettingsConfigDict(env_file=".env", env_file_encoding="utf-8")`
       - Add `@lru_cache` helper: `def get_settings() -> Settings`

    4. Create `backend/config/.env.example`:
       ```
       ANTHROPIC_API_KEY=sk-ant-...
       OPENAI_API_KEY=sk-...
       SUPABASE_URL=https://xxx.supabase.co
       SUPABASE_KEY=eyJ...
       ```
  </action>
  <verify>
    - `python -c "from backend.config.settings import Settings; print('Settings OK')"` succeeds
    - `pip install -r requirements.txt` completes without errors
    - All package directories have `__init__.py` files
  </verify>
  <done>
    Settings class loads from .env with type validation. All directories importable. Dependencies installable.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Pydantic schemas, async clients, and utility functions</name>
  <files>
    backend/llm/config.py
    backend/llm/schemas.py
    backend/llm/clients.py
    backend/llm/utils.py
  </files>
  <action>
    1. Create `backend/llm/config.py` with LLM-specific constants:
       - `NARRATIVE_MAX_ARTICLES = 20` (max articles per batch)
       - `NARRATIVE_ARTICLE_SNIPPET_LEN = 500` (chars per article in prompt)
       - `CLASSIFICATION_CATEGORIES` list: convoy, naval, flight, restricted_zone, not_relevant
       - `ENTITY_TYPES` list: military_unit, equipment, location, timestamp
       - `THREAT_LEVELS` dict mapping score ranges to GREEN/AMBER/RED

    2. Create `backend/llm/schemas.py` with ALL Pydantic output models:

       - `NarrativeCoordination(BaseModel)`:
         - `coordination_score: int = Field(ge=0, le=100)`
         - `synchronized_phrases: List[str]`
         - `outlet_count: int`
         - `geographic_focus: str`
         - `themes: List[str]` (narrative themes detected)
         - `confidence: int = Field(ge=0, le=100)`

       - `MovementCategory(str, Enum)`: convoy, naval, flight, restricted_zone, not_relevant

       - `PostClassification(BaseModel)`:
         - `category: MovementCategory`
         - `location: Optional[str]`
         - `confidence: int = Field(ge=0, le=100)`
         - `reasoning: str`

       - `ExtractedEntity(BaseModel)`:
         - `entity_type: str` (military_unit, equipment, location, timestamp)
         - `entity_value: str`
         - `source_span: str`
         - `latitude: Optional[float]`
         - `longitude: Optional[float]`
         - `confidence: int = Field(ge=0, le=100)`

       - `EntityExtraction(BaseModel)`:
         - `entities: List[ExtractedEntity]`

       - `ThreatLevel(str, Enum)`: GREEN, AMBER, RED

       - `IntelligenceBrief(BaseModel)`:
         - `threat_level: ThreatLevel`
         - `confidence: int = Field(ge=0, le=100)`
         - `summary: str`
         - `evidence_chain: List[str]`
         - `timeline: str`
         - `information_gaps: List[str]`
         - `collection_priorities: List[str]`

    3. Create `backend/llm/clients.py`:
       - `get_anthropic_client() -> AsyncAnthropic` — uses settings.anthropic_api_key
       - `get_openai_client() -> AsyncOpenAI` — uses settings.openai_api_key
       - Both import from `backend.config.settings.get_settings()`

    4. Create `backend/llm/utils.py`:
       - `create_retry_decorator()` — returns tenacity retry configured for:
         - `retry_if_exception_type((RateLimitError, APITimeoutError, InternalServerError))` for Anthropic
         - `retry_if_exception_type((openai.RateLimitError, openai.APITimeoutError, openai.InternalServerError))` for OpenAI
         - `stop_after_attempt(3)`
         - `wait_exponential(multiplier=1, min=2, max=60)`
       - `create_semaphore(max_concurrent: int) -> asyncio.Semaphore`
       - `log_llm_call(model: str, input_tokens: int, output_tokens: int, duration_ms: int, cost_usd: float)` — structlog JSON logging
       - `truncate_text(text: str, max_chars: int) -> str` — truncate with "..." suffix
       - `estimate_cost(model: str, input_tokens: int, output_tokens: int) -> float` — cost lookup for claude-sonnet-4-5 ($3/$15 per 1M) and gpt-4o-mini ($0.15/$0.60 per 1M)
  </action>
  <verify>
    - `python -c "from backend.llm.schemas import NarrativeCoordination, PostClassification, EntityExtraction, IntelligenceBrief; print('Schemas OK')"` succeeds
    - `python -c "from backend.llm.schemas import NarrativeCoordination; NarrativeCoordination(coordination_score=50, synchronized_phrases=['test'], outlet_count=3, geographic_focus='Taiwan Strait', themes=['military'], confidence=80); print('Validation OK')"` succeeds
    - `python -c "from backend.llm.schemas import NarrativeCoordination; NarrativeCoordination(coordination_score=150, synchronized_phrases=[], outlet_count=0, geographic_focus='', themes=[], confidence=0)"` raises ValidationError (score > 100)
    - `python -c "from backend.llm.utils import truncate_text, estimate_cost; assert truncate_text('hello world', 5) == 'he...'; print('Utils OK')"` succeeds
  </verify>
  <done>
    All 4 Pydantic output schemas validate with constraints. Client factories return async clients. Retry decorator, semaphore factory, logging, truncation, and cost estimation utilities work correctly.
  </done>
</task>

</tasks>

<verification>
- All imports work: `python -c "from backend.llm import schemas, clients, utils, config; from backend.config import settings; print('All imports OK')"`
- Pydantic validation enforces constraints (score ranges, enums)
- Settings class raises clear error when API keys missing
- No hardcoded API keys in any file
- `requirements.txt` installable
</verification>

<success_criteria>
- `backend/llm/` module exists with config.py, schemas.py, clients.py, utils.py
- `backend/config/settings.py` loads from .env with pydantic-settings
- All 4 Pydantic schemas (NarrativeCoordination, PostClassification, EntityExtraction, IntelligenceBrief) validate correctly
- Retry, rate limiting, logging, cost estimation utilities are importable and functional
- requirements.txt includes all Phase 2 dependencies
</success_criteria>

<output>
After completion, create `.planning/phases/02-intelligence-processing/02-01-SUMMARY.md`
</output>

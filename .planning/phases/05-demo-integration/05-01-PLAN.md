---
phase: 05-demo-integration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/generate_demo_fixture.py
  - scripts/demo_fixture.json
autonomous: true

must_haves:
  truths:
    - "Pre-computed demo data covers all 7 Supabase tables with proper time sequencing"
    - "Data follows 5-beat narrative arc: baseline, first signals, coordination detected, movement confirmed, full alert"
    - "Pre-computed demo data is complete and ready for playback without requiring live LLM processing"
  artifacts:
    - path: "scripts/generate_demo_fixture.py"
      provides: "Fixture generator that creates all demo data"
      min_lines: 200
    - path: "scripts/demo_fixture.json"
      provides: "Complete pre-computed demo dataset for all 7 tables"
      contains: "narrative_events"
  key_links:
    - from: "scripts/generate_demo_fixture.py"
      to: "scripts/demo_fixture.json"
      via: "JSON file write"
      pattern: "json\\.dump"
    - from: "scripts/demo_fixture.json"
      to: "src/models/schemas.py"
      via: "Schema compliance"
      pattern: "articles.*narrative_events.*movement_events.*alerts.*briefs"
---

<objective>
Generate the complete pre-computed demo dataset as a JSON fixture file covering all 7 Supabase tables with precise timing for the 72-hour Taiwan Strait escalation scenario.

Purpose: The demo playback engine (Plan 02) needs a single fixture file containing ALL data it will drip-feed into Supabase. This includes the existing Phase 1 raw data (articles, social_posts, vessel_positions) PLUS the processed intelligence outputs (narrative_events, movement_events, alerts, briefs) that would normally come from LLM processing. Pre-computing everything means zero live API calls during demo.

Output: `scripts/generate_demo_fixture.py` (generator) and `scripts/demo_fixture.json` (fixture data)
</objective>

<execution_context>
@/Users/gremmy/.claude/get-shit-done/workflows/execute-plan.md
@/Users/gremmy/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-demo-integration/05-CONTEXT.md

# Key source files for data patterns
@scripts/load_demo_data.py
@scripts/run_correlation_demo.py
@src/models/schemas.py
@frontend/src/types/database.ts
@src/models/threat_levels.py
@src/models/correlation.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create demo fixture generator with all 7 table types</name>
  <files>scripts/generate_demo_fixture.py, scripts/demo_fixture.json</files>
  <action>
Create `scripts/generate_demo_fixture.py` that generates a complete JSON fixture file for the demo playback engine.

The fixture must contain data for ALL 7 tables, time-sequenced across the 72-hour scenario with BASE_TIME = 2026-02-05T00:00:00Z. Each record has a `_demo_offset_seconds` field indicating when it should be inserted relative to demo start (0 = beginning, 300 = end at 5-min normal speed).

**Reuse existing data generators:** Import and call `generate_demo_articles()`, `generate_demo_posts()`, `generate_demo_positions()` from `scripts/load_demo_data.py` for the raw data (articles, social_posts, vessel_positions).

**Add NEW pre-computed data for processed tables:**

1. **narrative_events** (8-10 events across the 5 beats):
   - Beat 1 (baseline, T+0-14h): 1 event, coordination_score 10-15, outlet_count 1-2, themes=["cooperation"], confidence 30-40
   - Beat 2 (first signals, T+14-28h): 2 events, coordination_score 30-45, outlet_count 2-3, themes=["sovereignty"], confidence 50-60
   - Beat 3 (coordination detected, T+28-42h): 2 events, coordination_score 55-70, outlet_count 3, themes=["reunification","sovereignty"], confidence 65-75
   - Beat 4 (movement confirmed, T+42-56h): 2 events, coordination_score 70-80, outlet_count 3-4, themes=["military","exercises"], confidence 75-85
   - Beat 5 (full alert, T+56-72h): 2 events, coordination_score 85-95, outlet_count 4, themes=["military","readiness","decisive_action"], confidence 85-95

   Each narrative_event has: coordination_score, outlet_count, synchronized_phrases (list of strings), geographic_focus ("Taiwan Strait"), themes (list), confidence, article_ids (reference article IDs from the articles list), created_at (ISO string).

   Also include fields that match the Supabase schema for the narrative_events table: event_type (string like "coordination_spike"), summary (string describing what was detected), source_ids (list of article IDs).

2. **movement_events** (12-15 events):
   - Beat 1: 1-2 events, category="naval", confidence 25-35
   - Beat 2: 2-3 events, categories=["naval","convoy"], confidence 45-60
   - Beat 3: 2-3 events, categories=["convoy","restricted_zone"], confidence 60-75
   - Beat 4: 3-4 events, categories=["naval","convoy","flight","restricted_zone"], confidence 75-90
   - Beat 5: 3-4 events, all categories, confidence 85-95

   Each has: category, location_name ("Taiwan Strait"), location_lat (23.5-25.5 range), location_lon (118.5-121.0 range), confidence, source_post_ids (reference post IDs), created_at.

   Also include fields matching schema: event_type (string like "military_movement"), vessel_mmsi (nullable int), description (string).

3. **alerts** (5 alert states, one per beat -- each UPDATE overwrites the previous):
   - Beat 1: threat_level="GREEN", threat_score=12, confidence=30, region="Taiwan Strait"
   - Beat 2: threat_level="GREEN", threat_score=28, confidence=50
   - Beat 3: threat_level="AMBER", threat_score=52, confidence=70
   - Beat 4: threat_level="AMBER", threat_score=68, confidence=80
   - Beat 5: threat_level="RED", threat_score=82, confidence=92

   Each alert has: region, threat_level, threat_score, confidence, sub_scores (dict with outlet_score, phrase_score, volume_score, geo_score), correlation_metadata (dict with narrative_event_ids, movement_event_ids, evidence_summary, detection_history), updated_at.

   IMPORTANT: Alerts use upsert logic -- the first alert is an INSERT, subsequent ones are UPDATEs to the same row. The fixture should mark alerts with `_demo_action: "insert"` for the first and `_demo_action: "update"` for subsequent ones.

4. **briefs** (3 briefs at key escalation moments):
   - Beat 2 brief (T+~28h): threat_level="GREEN", confidence=50, summary about increased rhetoric
   - Beat 4 brief (T+~48h): threat_level="AMBER", confidence=80, summary about coordinated messaging + military movement
   - Beat 5 brief (T+~68h): threat_level="RED", confidence=92, summary about imminent threat assessment

   Each brief has: threat_level, confidence, summary (2-3 sentences), evidence_chain (list of strings), timeline (string), information_gaps (list), collection_priorities (list), narrative_event_ids, movement_event_ids, created_at.

**Timing calculation:** Map 72 simulated hours to 300 seconds (5 min) for Normal speed. So 1 simulated hour = 300/72 = 4.167 seconds. Each record's `_demo_offset_seconds` = (record's simulated hour / 72) * 300.

**Output format:** Write JSON with structure:
```json
{
  "metadata": {
    "generated_at": "...",
    "base_time": "2026-02-05T00:00:00Z",
    "simulated_hours": 72,
    "normal_duration_seconds": 300,
    "total_records": N,
    "record_counts": {"articles": 60, "social_posts": 120, ...}
  },
  "records": [
    {
      "_table": "articles",
      "_demo_offset_seconds": 0.5,
      "_demo_action": "insert",
      "data": { ...article fields... }
    },
    ...
  ]
}
```

The `records` array should be SORTED by `_demo_offset_seconds` so the playback engine can simply iterate through in order.

**Schema validation during generation:** Import and validate generated records against Pydantic schemas from `src/models/schemas.py` (e.g. `NarrativeEventCreate`, `MovementEventCreate`, `AlertCreate`, `BriefCreate`) where applicable. This ensures the fixture data is type-safe and compatible with Supabase table schemas before writing to JSON.

**Continuous drip pacing:** Add small random jitter (0-2 seconds) to each record's offset to prevent multiple records landing at exact same time. Records should feel like they trickle in naturally.

Run the generator to produce `scripts/demo_fixture.json`.
  </action>
  <verify>
Run: `python scripts/generate_demo_fixture.py`
Verify: `scripts/demo_fixture.json` exists and is valid JSON.
Check with Python:
```python
import json
with open("scripts/demo_fixture.json") as f:
    data = json.load(f)
assert data["metadata"]["total_records"] > 200
assert all(t in [r["_table"] for r in data["records"]] for t in ["articles", "social_posts", "vessel_positions", "narrative_events", "movement_events", "alerts", "briefs"])
assert data["records"] == sorted(data["records"], key=lambda r: r["_demo_offset_seconds"])
```
  </verify>
  <done>
`scripts/demo_fixture.json` contains 200+ records across all 7 tables, sorted by demo offset, with proper 5-beat narrative arc timing. Generator is rerunnable.
  </done>
</task>

<task type="auto">
  <name>Task 2: Validate fixture data against Supabase schema compatibility</name>
  <files>scripts/generate_demo_fixture.py</files>
  <action>
Add a `--validate` flag to the generator script that:

1. Loads the fixture JSON
2. Groups records by table
3. Validates each record's `data` field has the required columns for its table:
   - articles: url, title, domain, published_at, tone_score
   - social_posts: telegram_id, channel, text, timestamp, views
   - vessel_positions: mmsi, ship_name, latitude, longitude, speed, course, timestamp
   - narrative_events: event_type, summary, confidence, source_ids (also coordination_score, outlet_count, synchronized_phrases, geographic_focus, themes, article_ids, created_at)
   - movement_events: event_type, description, location_lat, location_lon, confidence (also category, location_name, source_post_ids, created_at)
   - alerts: region, threat_level, threat_score, confidence, sub_scores, correlation_metadata
   - briefs: threat_level, confidence, summary, evidence_chain, timeline, collection_priorities, narrative_event_ids, movement_event_ids

4. Validates the 5-beat arc structure:
   - Alert records escalate GREEN -> GREEN -> AMBER -> AMBER -> RED
   - No alert has threat_score above its beat's expected range
   - Brief threat_levels match their beat's alert level

5. Validates timing:
   - All offsets are between 0 and 300 seconds
   - Records are sorted by offset
   - No two records have identical offset (jitter applied)

6. Prints summary: record counts per table, beat timing ranges, validation PASS/FAIL

Run validation: `python scripts/generate_demo_fixture.py --validate`
  </action>
  <verify>
Run `python scripts/generate_demo_fixture.py --validate` and confirm output shows all checks PASS with no validation errors. Expected output includes record counts for all 7 tables.
  </verify>
  <done>
Fixture validation passes with all 7 table types present, 5-beat arc verified, timing sorted and within bounds, all records schema-compliant.
  </done>
</task>

</tasks>

<verification>
1. `scripts/demo_fixture.json` exists and is valid JSON
2. Contains records for all 7 tables: articles, social_posts, vessel_positions, narrative_events, movement_events, alerts, briefs
3. Records sorted by `_demo_offset_seconds` (0 to ~300)
4. Alert escalation follows GREEN -> GREEN -> AMBER -> AMBER -> RED
5. `python scripts/generate_demo_fixture.py --validate` passes all checks
</verification>

<success_criteria>
- Complete demo fixture with 200+ records across 7 tables
- 5-beat narrative arc with proper escalation
- All records schema-compatible with Supabase tables
- Validation script confirms data integrity
- No live API calls needed to generate fixture
</success_criteria>

<output>
After completion, create `.planning/phases/05-demo-integration/05-01-SUMMARY.md`
</output>

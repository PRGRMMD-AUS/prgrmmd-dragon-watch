---
phase: 03-correlation-engine
plan: 02
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src/processors/correlate_events.py
autonomous: true

must_haves:
  truths:
    - "Narrative events and movement events within 72-hour window are matched as correlated pairs"
    - "Movement events are checked for geographic proximity to Taiwan Strait via containment test"
    - "Composite threat score (0-100) is calculated from weighted sub-factors: outlet count (0.30), phrase novelty (0.25), post volume (0.20), geographic proximity (0.25)"
    - "Existing alert is updated (not duplicated) with monotonic escalation -- never de-escalates from AMBER to GREEN"
    - "Each alert contains correlation_metadata linking to specific narrative_event_ids and movement_event_ids"
  artifacts:
    - path: "src/processors/correlate_events.py"
      provides: "Complete correlation engine with time-window matching, geo scoring, composite scoring, and alert upsert"
      exports: ["correlate_events_batch"]
      min_lines: 150
  key_links:
    - from: "src/processors/correlate_events.py"
      to: "src/database/client.py"
      via: "async Supabase client for reading events and writing alerts"
      pattern: "get_supabase|await client\\.table"
    - from: "src/processors/correlate_events.py"
      to: "src/models/threat_levels.py"
      via: "ThreatLevel enum for scoring and monotonic enforcement"
      pattern: "ThreatLevel|determine_threat_level|can_transition_to"
    - from: "src/processors/correlate_events.py"
      to: "src/utils/geo_utils.py"
      via: "Geographic containment check for movement events"
      pattern: "is_in_taiwan_strait|check_narrative_geo_match|normalize_min_max"
    - from: "src/processors/correlate_events.py"
      to: "Supabase alerts table"
      via: "upsert with monotonic escalation check"
      pattern: "client\\.table\\(\"alerts\"\\)"
---

<objective>
Build the core correlation engine that matches narrative coordination events with movement events by time and geography, calculates composite threat scores with sub-factor breakdown, and writes/updates a single escalating alert to Supabase.

Purpose: This is the heart of Dragon Watch -- the correlation of two independent intelligence streams to produce actionable threat assessments. Without this, narrative analysis and movement classification are isolated streams with no convergence signal.

Output: `src/processors/correlate_events.py` with complete batch correlation pipeline following the established processor pattern (batch_articles.py, batch_posts.py).
</objective>

<execution_context>
@/Users/gremmy/.claude/get-shit-done/workflows/execute-plan.md
@/Users/gremmy/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/03-correlation-engine/03-CONTEXT.md
@.planning/phases/03-correlation-engine/03-RESEARCH.md
@.planning/phases/03-correlation-engine/03-01-SUMMARY.md

@src/processors/batch_articles.py
@src/processors/batch_posts.py
@src/processors/brief_generator.py
@src/database/client.py
@src/models/threat_levels.py
@src/models/correlation.py
@src/utils/geo_utils.py
@src/llm/schemas.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build correlation matching and scoring engine</name>
  <files>src/processors/correlate_events.py</files>
  <action>
Create `src/processors/correlate_events.py` following the established batch processor pattern from `batch_articles.py` and `batch_posts.py`.

**Imports and setup:**
- asyncio, structlog, datetime/timedelta/timezone
- from src.database.client import get_supabase
- from src.models.threat_levels import ThreatLevel, determine_threat_level, calculate_confidence, GREEN_THRESHOLD, RED_THRESHOLD
- from src.models.correlation import CorrelationResult, SubScores, AlertUpsertData
- from src.utils.geo_utils import is_in_taiwan_strait, check_narrative_geo_match, normalize_min_max
- logger = structlog.get_logger()

**Constants (top of file, NOT scattered):**
```python
CORRELATION_WINDOW_HOURS = 72
WEIGHTS = {"outlet": 0.30, "phrase": 0.25, "volume": 0.20, "geo": 0.25}
# Normalization ranges (tuned for demo data: 60 articles, 120 posts, 4 outlets)
OUTLET_MIN, OUTLET_MAX = 1, 4
PHRASE_MIN, PHRASE_MAX = 0, 10
VOLUME_MIN, VOLUME_MAX = 0, 50
```

**Function 1: `fetch_narrative_events(hours: int = 72)`**
- Async. Get Supabase client via `await get_supabase()`
- Query `narrative_events` table, select all, ordered by `detected_at` desc (or `created_at` -- check what the existing write_narrative_event in batch_articles.py uses; it writes `created_at`)
- Use `.gte("created_at", cutoff.isoformat())` where cutoff = datetime.now(tz=timezone.utc) - timedelta(hours=hours)
- Return response.data

**Function 2: `fetch_movement_events(hours: int = 72)`**
- Async. Same pattern as above but for `movement_events` table
- The batch_posts.py write_movement_event writes `created_at`
- Return response.data

**Function 3: `match_events_by_time_window(narrative_events, movement_events, window_hours=72)`**
- For each narrative event, find all movement events within the time window
- Parse timestamps: `datetime.fromisoformat(ts.replace('Z', '+00:00'))` to handle both 'Z' and '+00:00' suffixes
- Use narrative event's `created_at` as reference time
- Return list of dicts: `{"narrative": narr_event, "movements": [list of matched movement events]}`
- Skip narrative events that match zero movement events

**Function 4: `calculate_composite_score(narrative_event, matched_movements, geo_match: bool) -> tuple[float, SubScores]`**
- Extract raw values:
  - outlet_count: narrative_event.get("outlet_count", 1)
  - phrase_count: len(narrative_event.get("synchronized_phrases", []))
  - post_volume: len(matched_movements)
  - geo_proximity: 100.0 if geo_match else 0.0
- Normalize using `normalize_min_max` from geo_utils:
  - outlet_score = normalize_min_max(outlet_count, OUTLET_MIN, OUTLET_MAX)
  - phrase_score = normalize_min_max(phrase_count, PHRASE_MIN, PHRASE_MAX)
  - volume_score = normalize_min_max(post_volume, VOLUME_MIN, VOLUME_MAX)
  - geo_score = geo_proximity (already 0-100)
- Calculate weighted composite:
  ```
  composite = (outlet_score * WEIGHTS["outlet"] + phrase_score * WEIGHTS["phrase"] +
               volume_score * WEIGHTS["volume"] + geo_score * WEIGHTS["geo"])
  ```
- Return (composite, SubScores(outlet_score=..., phrase_score=..., volume_score=..., geo_score=...))

**Function 5: `build_evidence_summary(narrative_event, matched_movements) -> str`**
- Generate plain-English one-liner:
  - f"{outlet_count} state media outlets detected coordinating on '{geographic_focus}' themes, correlating with {len(matched_movements)} civilian movement reports in region."
- Keep it concise -- the brief generator (Phase 2) handles full narrative

**Function 6: `upsert_alert(correlation: CorrelationResult)`**
- Async. Get Supabase client
- Query existing alert: `client.table("alerts").select("*").eq("region", "Taiwan Strait").is_("resolved_at", "null").execute()`
- If existing alert found:
  - Parse current threat level from existing alert
  - Use ThreatLevel.can_transition_to() to check if new level is higher or equal
  - If new level is LOWER than current: log warning, keep current level (monotonic enforcement)
  - If new level is HIGHER or equal: update the alert with new threat_score, threat_level, confidence, sub_scores, and APPEND to correlation_metadata.detection_history
  - Update via: `client.table("alerts").update({...}).eq("id", alert_id).execute()`
- If no existing alert:
  - Insert new alert with: region, threat_level (name string), threat_score, confidence, sub_scores (as dict), correlation_metadata (as dict with narrative_event_ids, movement_event_ids, evidence_summary, detection_history=[{detected_at, score, level}])
  - Insert via: `client.table("alerts").insert({...}).execute()`
- Note: The existing AlertCreate schema in schemas.py uses severity "low/medium/high/critical" -- but the CONTEXT.md and roadmap specify GREEN/AMBER/RED. The alert upsert should write the fields that the alerts table actually has. Write `threat_level` as a string field. If the table schema doesn't have these columns, the Supabase insert will fail gracefully. The executor should check the actual table schema and add columns if needed via Supabase SQL editor note.
- IMPORTANT: Store the alert data in a way that's compatible with whatever columns exist. Use title/description from AlertCreate pattern if needed, but also include threat_level and correlation_metadata as additional fields. The executor will adapt to the actual schema.

**Function 7: `correlate_events_batch() -> dict`**
- Main pipeline orchestrator (matches pattern from batch_articles.py)
- logger.info("pipeline_start", pipeline="correlate_events")
- Fetch both event streams concurrently: asyncio.gather(fetch_narrative_events(), fetch_movement_events())
- If either stream is empty, log and return early with status
- Match events by time window
- For each match:
  - Check geographic proximity:
    - Narrative side: check_narrative_geo_match(narrative.get("geographic_focus"))
    - Movement side: check if ANY matched movement has location coords in Taiwan Strait via is_in_taiwan_strait(lat, lon)
    - geo_match = narrative_geo AND movement_geo (both must be True)
  - Calculate composite score and sub-scores
  - Build evidence summary
  - Create CorrelationResult
- Take the correlation with the HIGHEST composite score (or the most recent one if scores are equal)
- Determine threat level from highest score
- Calculate confidence
- Upsert alert
- Return summary dict with: status, correlations_found, highest_score, threat_level, confidence

Add `if __name__ == "__main__": asyncio.run(correlate_events_batch())` for standalone testing.
  </action>
  <verify>
Run: `python -c "from src.processors.correlate_events import correlate_events_batch, fetch_narrative_events, fetch_movement_events, match_events_by_time_window, calculate_composite_score, upsert_alert; print('Correlation engine imports OK')"`

Run: `python -c "
from src.processors.correlate_events import calculate_composite_score, match_events_by_time_window
from src.models.correlation import SubScores

# Test scoring with mock data
narr = {'outlet_count': 3, 'synchronized_phrases': ['phrase1', 'phrase2', 'phrase3'], 'geographic_focus': 'Taiwan Strait'}
movements = [{'id': 1}, {'id': 2}, {'id': 3}]
score, sub = calculate_composite_score(narr, movements, geo_match=True)
print(f'Score: {score:.1f}, Outlet: {sub.outlet_score:.0f}, Phrase: {sub.phrase_score:.0f}, Volume: {sub.volume_score:.0f}, Geo: {sub.geo_score:.0f}')
assert 40 < score < 80, f'Expected 40-80, got {score}'
print('Scoring test PASSED')
"
`
  </verify>
  <done>
correlate_events.py exists with all 7 functions. Time-window matching filters events within 72 hours. Geographic matching checks both narrative focus and movement coordinates. Composite scoring uses weighted sub-factors with min-max normalization. Alert upsert enforces monotonic escalation. Pipeline follows established batch processor pattern with structured logging. Can be run standalone via `python -m src.processors.correlate_events`.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add correlation endpoint to FastAPI app</name>
  <files>src/main.py</files>
  <action>
Read the existing `src/main.py` (FastAPI app from Phase 1, Plan 04) to understand the current endpoint structure.

Add a new POST endpoint `/api/correlate` that:
1. Calls `correlate_events_batch()` from `src/processors/correlate_events`
2. Returns the result dict as JSON
3. Follow the same pattern as existing endpoints (e.g., the article processing or brief generation endpoints)

Also add a GET endpoint `/api/alerts/current` that:
1. Queries the alerts table for the current active (unresolved) Taiwan Strait alert
2. Returns the alert data including threat_level, threat_score, confidence, sub_scores, correlation_metadata
3. Returns 404 if no active alert exists

Import from correlate_events at the top of main.py. Use the same async pattern as other endpoints.
  </action>
  <verify>
Run: `python -c "from src.main import app; routes = [r.path for r in app.routes]; print('/api/correlate' in routes or any('correlate' in str(r.path) for r in app.routes)); print('Endpoint added')"`
  </verify>
  <done>
POST /api/correlate endpoint exists and calls correlate_events_batch(). GET /api/alerts/current endpoint exists and returns the active alert. Both endpoints follow existing FastAPI patterns in the app.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from src.processors.correlate_events import correlate_events_batch; print('OK')"` -- imports clean
2. Scoring produces expected values for known inputs (3 outlets, 3 phrases, 3 movements, geo_match=True should score 40-80)
3. Monotonic enforcement: ThreatLevel.RED.can_transition_to(ThreatLevel.GREEN) returns False
4. FastAPI app starts without import errors: `timeout 5 uvicorn src.main:app --host 0.0.0.0 --port 8001 || true` (expect startup, not crash)
5. All functions in correlate_events.py are async where they touch Supabase
</verification>

<success_criteria>
- Correlation engine fetches both event streams from Supabase within 72h window
- Time-window matching pairs narrative events with temporally proximate movement events
- Geographic proximity check validates both narrative focus AND movement coordinates
- Composite score calculated with 4 weighted sub-factors, all normalized to 0-100
- Alert upserted with monotonic escalation (never de-escalates)
- Evidence chain preserved in correlation_metadata JSONB
- FastAPI endpoints wired for triggering correlation and reading current alert
- Standalone execution via `python -m src.processors.correlate_events` works
</success_criteria>

<output>
After completion, create `.planning/phases/03-correlation-engine/03-02-SUMMARY.md`
</output>

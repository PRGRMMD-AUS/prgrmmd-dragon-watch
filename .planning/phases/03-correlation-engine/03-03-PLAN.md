---
phase: 03-correlation-engine
plan: 03
type: execute
wave: 3
depends_on: ["03-02"]
files_modified:
  - scripts/verify_phase_3.py
  - scripts/run_correlation_demo.py
autonomous: true

must_haves:
  truths:
    - "Running correlation against demo data produces at least one alert with threat_level transitioning from GREEN through AMBER to RED"
    - "Sub-scores are visible and non-zero for correlated events"
    - "Evidence chain in alert links back to specific narrative_event_ids and movement_event_ids"
    - "Confidence score accompanies each threat assessment (0-95 range)"
    - "Alert consolidates into a single escalating record (not multiple separate alerts)"
  artifacts:
    - path: "scripts/run_correlation_demo.py"
      provides: "End-to-end demo that processes narrative+movement events through correlation engine"
      min_lines: 50
    - path: "scripts/verify_phase_3.py"
      provides: "Verification script checking all Phase 3 requirements"
      min_lines: 80
  key_links:
    - from: "scripts/run_correlation_demo.py"
      to: "src/processors/correlate_events.py"
      via: "calls correlate_events_batch()"
      pattern: "correlate_events_batch"
    - from: "scripts/run_correlation_demo.py"
      to: "Supabase alerts table"
      via: "reads back alert to verify"
      pattern: "client\\.table\\(\"alerts\"\\)"
---

<objective>
Create end-to-end verification and demo scripts that prove the correlation engine works against the existing demo dataset, producing escalating alerts with evidence chains.

Purpose: Phase 3 cannot be considered complete without proving that the correlation engine produces correct, escalating threat assessments when run against the 72-hour Taiwan Strait demo data. This plan creates the scripts that both verify correctness and serve as a demonstration of the complete pipeline.

Output: A demo runner script and a verification script that together prove all CORR-01 through CORR-04 requirements are met.
</objective>

<execution_context>
@/Users/gremmy/.claude/get-shit-done/workflows/execute-plan.md
@/Users/gremmy/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/03-correlation-engine/03-CONTEXT.md
@.planning/phases/03-correlation-engine/03-02-SUMMARY.md

@src/processors/correlate_events.py
@src/models/threat_levels.py
@src/models/correlation.py
@scripts/load_demo_data.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create correlation demo runner</name>
  <files>scripts/run_correlation_demo.py</files>
  <action>
Create `scripts/run_correlation_demo.py` -- a script that demonstrates the full correlation pipeline working against demo data.

The demo data (loaded by `scripts/load_demo_data.py`) has:
- 60 articles across 3 phases (GREEN 0-24h, AMBER 24-48h, RED 48-72h)
- 120 social posts across 3 phases
- Narrative events and movement events written by Phase 2 processors (batch_articles.py, batch_posts.py)

However, the demo data may NOT have been processed through the Phase 2 LLM pipeline yet (that requires API keys). So this script needs to handle two scenarios:

**Scenario A: Processed events exist** (narrative_events and movement_events tables have data)
- Fetch events from both tables
- Run correlate_events_batch()
- Print results

**Scenario B: No processed events exist** (tables empty because LLM pipeline hasn't run)
- Insert synthetic narrative_events and movement_events directly into Supabase, simulating what the Phase 2 pipeline would produce
- Synthetic narrative events (3 events matching the 3 demo phases):
  1. GREEN phase event: coordination_score=15, outlet_count=2, synchronized_phrases=["peaceful development", "cross-strait"], geographic_focus="Taiwan Strait", themes=["cooperation"], confidence=40, article_ids=[1,2,3], created_at=BASE_TIME+12h
  2. AMBER phase event: coordination_score=55, outlet_count=3, synchronized_phrases=["reunification inevitable", "sovereignty non-negotiable", "separatist forces"], geographic_focus="Taiwan Strait", themes=["sovereignty", "reunification"], confidence=70, article_ids=[21,22,23], created_at=BASE_TIME+36h
  3. RED phase event: coordination_score=85, outlet_count=4, synchronized_phrases=["live-fire exercises", "combat readiness", "decisive action", "safeguard sovereignty"], geographic_focus="Taiwan Strait", themes=["military", "exercises", "readiness"], confidence=90, article_ids=[41,42,43], created_at=BASE_TIME+60h

- Synthetic movement events (9 events across phases):
  GREEN: 2 events (category="naval", location="Taiwan Strait", confidence=30, location coords within strait, created_at around 12-20h)
  AMBER: 3 events (category="convoy", "naval", "restricted_zone", confidence=60-75, coords in strait, created_at around 28-40h)
  RED: 4 events (category="naval", "convoy", "flight", "restricted_zone", confidence=80-95, coords in strait, created_at around 50-65h)

- After inserting synthetic events, run correlate_events_batch()

**Output format:**
Print structured output showing:
- Events found (narrative count, movement count)
- Correlations detected
- For each correlation: sub-scores breakdown, composite score, threat level, confidence
- Current alert state after correlation
- Evidence chain summary

Use BASE_TIME = datetime(2026, 2, 5, 0, 0, 0) matching load_demo_data.py.
Add `if __name__ == "__main__": asyncio.run(main())` for direct execution.
  </action>
  <verify>
Run: `python -c "from scripts.run_correlation_demo import main; print('Demo script imports OK')"`
  </verify>
  <done>
Demo runner script exists, handles both processed-events and synthetic-events scenarios. Can be executed standalone to demonstrate the full correlation pipeline producing escalating GREEN->AMBER->RED alerts.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Phase 3 verification script</name>
  <files>scripts/verify_phase_3.py</files>
  <action>
Create `scripts/verify_phase_3.py` that programmatically verifies all Phase 3 requirements.

**Structure:** A series of checks that print PASS/FAIL for each requirement.

**Check 1: CORR-01 - Time-window matching**
- Import match_events_by_time_window from correlate_events
- Create mock narrative events and movement events with known timestamps
- Verify that events within 72h window are matched
- Verify that events outside 72h window are NOT matched
- Print: "CORR-01 Time-window matching: PASS/FAIL"

**Check 2: CORR-02 - Geographic proximity scoring**
- Import is_in_taiwan_strait, check_narrative_geo_match from geo_utils
- Test point inside strait (24.5, 120.0) -> True
- Test point outside strait (10.0, 100.0) -> False
- Test narrative focus "Taiwan Strait" -> True
- Test narrative focus "South China Sea" -> False (not in our region)
- Print: "CORR-02 Geographic proximity: PASS/FAIL"

**Check 3: CORR-03 - Threat level calculation**
- Import determine_threat_level, calculate_composite_score
- Test score 15 -> GREEN
- Test score 50 -> AMBER
- Test score 85 -> RED
- Test composite scoring with known inputs produces expected range
- Verify sub-scores are non-negative
- Print: "CORR-03 Threat level calculation: PASS/FAIL"

**Check 4: CORR-04 - Evidence chain**
- Import CorrelationResult from models
- Create a CorrelationResult with narrative_event_ids and movement_event_ids
- Verify both ID lists are non-empty
- Verify evidence_summary is a non-empty string
- Print: "CORR-04 Evidence chain: PASS/FAIL"

**Check 5: Monotonic escalation**
- Import ThreatLevel
- Verify GREEN.can_transition_to(AMBER) == True
- Verify AMBER.can_transition_to(RED) == True
- Verify RED.can_transition_to(AMBER) == False
- Verify RED.can_transition_to(GREEN) == False
- Print: "Monotonic escalation: PASS/FAIL"

**Check 6: Confidence score**
- Import calculate_confidence
- Verify result is int, 0-95 range
- Verify higher event counts produce higher confidence
- Print: "Confidence scoring: PASS/FAIL"

**Check 7: Module imports**
- Verify all Phase 3 modules import cleanly
- correlate_events, threat_levels, correlation models, geo_utils
- Print: "Module imports: PASS/FAIL"

**Summary:** Print total PASS/FAIL count and overall status.

These are pure logic checks that do NOT require Supabase connection or API keys. They test the correlation engine's algorithms directly using mock data.

Add `if __name__ == "__main__": main()` (synchronous, no async needed for pure logic tests).
  </action>
  <verify>
Run: `python scripts/verify_phase_3.py`
Expected: All 7 checks show PASS. Zero FAIL.
  </verify>
  <done>
Verification script exists and tests all CORR-01 through CORR-04 requirements plus monotonic escalation and confidence scoring. All checks pass against the correlation engine's pure logic (no Supabase needed). Script provides clear PASS/FAIL output for each requirement.
  </done>
</task>

</tasks>

<verification>
1. `python scripts/verify_phase_3.py` -- all 7 checks PASS
2. `python -c "from scripts.run_correlation_demo import main; print('OK')"` -- demo script imports
3. All CORR-01 through CORR-04 requirements verified programmatically
4. Monotonic escalation proven (RED cannot transition to GREEN/AMBER)
5. Evidence chain validated (correlation links to specific event IDs)
</verification>

<success_criteria>
- verify_phase_3.py runs and all 7 checks pass (CORR-01, CORR-02, CORR-03, CORR-04, monotonic escalation, confidence, module imports)
- run_correlation_demo.py can execute against demo data (or synthetic fallback) to demonstrate GREEN->AMBER->RED escalation
- No Supabase connection required for verification checks (pure logic tests)
- Scripts follow existing patterns (asyncio.run for async, structured output)
</success_criteria>

<output>
After completion, create `.planning/phases/03-correlation-engine/03-03-SUMMARY.md`
</output>

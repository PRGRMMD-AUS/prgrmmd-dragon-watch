---
phase: 01-foundation
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/fetchers/gdelt.py
  - src/fetchers/telegram.py
autonomous: true
user_setup:
  - service: telegram
    why: "Telegram MTProto API access for scraping public OSINT channels"
    env_vars:
      - name: TELEGRAM_API_ID
        source: "https://my.telegram.org/apps -> App api_id"
      - name: TELEGRAM_API_HASH
        source: "https://my.telegram.org/apps -> App api_hash"

must_haves:
  truths:
    - "GDELT fetcher queries Chinese state media domains and returns article data with tone scores"
    - "Telegram scraper connects to public OSINT channels and returns message data"
    - "Both fetchers validate data through Pydantic models before Supabase insert"
  artifacts:
    - path: "src/fetchers/gdelt.py"
      provides: "GDELT article fetcher for Chinese state media"
      contains: "domain_exact"
    - path: "src/fetchers/telegram.py"
      provides: "Telegram OSINT channel scraper"
      contains: "TelegramClient"
  key_links:
    - from: "src/fetchers/gdelt.py"
      to: "src/database/client.py"
      via: "get_supabase() for inserts"
      pattern: "get_supabase|supabase.*table.*articles.*insert"
    - from: "src/fetchers/gdelt.py"
      to: "src/models/schemas.py"
      via: "ArticleCreate for validation"
      pattern: "ArticleCreate"
    - from: "src/fetchers/telegram.py"
      to: "src/database/client.py"
      via: "get_supabase() for inserts"
      pattern: "get_supabase|supabase.*table.*social_posts.*insert"
    - from: "src/fetchers/telegram.py"
      to: "src/models/schemas.py"
      via: "SocialPostCreate for validation"
      pattern: "SocialPostCreate"
---

<objective>
Build the GDELT article fetcher and Telegram channel scraper -- the two text-based data ingestion pipelines. Both fetch data from external sources, validate through Pydantic models, and insert into Supabase.

Purpose: DATA-01 (GDELT state media articles) and DATA-02 (Telegram OSINT messages) are the raw intelligence that feeds Stream 1 (Narrative) and Stream 2 (Movement) in later phases. Without ingested text data, the LLM analysis pipeline has nothing to process.
Output: Two working fetcher modules that can be called independently or from FastAPI background tasks.
</objective>

<execution_context>
@/Users/gremmy/.claude/get-shit-done/workflows/execute-plan.md
@/Users/gremmy/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-foundation/01-RESEARCH.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: GDELT Chinese state media fetcher</name>
  <files>src/fetchers/gdelt.py</files>
  <action>
    Create the GDELT article fetcher that queries Chinese state media domains and writes to Supabase.

    Create `src/fetchers/gdelt.py` with:

    1. **Constants at top of file:**
       - STATE_MEDIA_DOMAINS: list of Chinese state media domains to query.
         Use `domain_exact` parameter (NOT `domain` -- partial matching causes false positives).
         Domains: ["xinhuanet.com", "globaltimes.cn", "cctv.com", "people.com.cn"]
         Add comment noting these need validation against GDELT (see research open question #1).

    2. **`async def fetch_gdelt_articles(lookback_hours: int = 24, max_records: int = 250) -> list[dict]`:**
       - Calculate start_date and end_date from lookback_hours (format: YYYY-MM-DD)
       - Create gdeltdoc.Filters with domain_exact=STATE_MEDIA_DOMAINS, start_date, end_date, num_records=max_records
       - Call gd.article_search(filters) -- NOTE: gdeltdoc is synchronous (returns pandas DataFrame), so wrap in asyncio.to_thread() to avoid blocking the event loop
       - Handle empty DataFrame (return empty list)
       - Convert DataFrame rows to ArticleCreate models:
         Map columns: url->url, title->title, domain->domain, seendate->published_at (parse to datetime), language->language, sourcecountry->source_country
         For tone_score: gdeltdoc does NOT return tone directly in article_search. Set tone_score=None for now (will be enriched in Phase 2). Add TODO comment.
         Store full row as raw_data dict for debugging
       - Return list of validated ArticleCreate.model_dump() dicts

    3. **`async def fetch_and_store_articles(lookback_hours: int = 24) -> int`:**
       - Call fetch_gdelt_articles()
       - Get supabase client via get_supabase()
       - Batch insert articles using supabase.table("articles").upsert(articles, on_conflict="url").execute()
         Use upsert on url to avoid duplicate errors on re-fetches
       - Return count of inserted articles
       - Add try/except with logging for Supabase errors
       - Add 2-second delay between GDELT queries if called in a loop (rate limit protection)

    4. **Error handling:**
       - Wrap GDELT API call in try/except, log and return empty list on failure
       - Log domain, date range, and result count for debugging

    IMPORTANT: Use `domain_exact` not `domain`. Use asyncio.to_thread() for the synchronous gdeltdoc call. The gdeltdoc library is sync-only.
  </action>
  <verify>
    - `python -c "from src.fetchers.gdelt import fetch_gdelt_articles, fetch_and_store_articles; print('GDELT fetcher imports OK')"`
    - Check that domain_exact is used: `grep "domain_exact" src/fetchers/gdelt.py`
    - Check asyncio.to_thread usage: `grep "to_thread" src/fetchers/gdelt.py`
  </verify>
  <done>
    GDELT fetcher imports cleanly, uses domain_exact for Chinese state media, wraps sync gdeltdoc in asyncio.to_thread, validates through ArticleCreate, upserts to Supabase articles table with conflict handling on url.
  </done>
</task>

<task type="auto">
  <name>Task 2: Telegram OSINT channel scraper</name>
  <files>src/fetchers/telegram.py</files>
  <action>
    Create the Telegram scraper that pulls messages from public OSINT/military channels and writes to Supabase.

    Create `src/fetchers/telegram.py` with:

    1. **Constants at top of file:**
       - OSINT_CHANNELS: list of public Telegram channels to scrape.
         Initial list: ["@osinttechnical", "@IntelDoge", "@militarymap", "@Lobaev_Z", "@ukraine_map"]
         Add comment: "Validate channel accessibility during setup. Add 5-10 more as needed."
         These are OSINT/military channels that post about global military movements.

    2. **Client initialization:**
       - Create module-level TelegramClient instance: `client = TelegramClient('dragon-watch', api_id, api_hash)`
       - Read TELEGRAM_API_ID (cast to int) and TELEGRAM_API_HASH from env vars
       - Set flood_sleep_threshold=60 (auto-sleep on FloodWaitError up to 60 seconds)

    3. **`async def scrape_channel(channel: str, limit: int = 100) -> list[dict]`:**
       - Iterate messages with client.iter_messages(channel, limit=limit)
       - For each message, create SocialPostCreate:
         telegram_id=message.id, channel=channel, text=message.text (skip if None/empty), timestamp=message.date (as datetime), views=message.views
         Store raw message data as raw_data dict (id, text, date, views, forward info if any)
       - Validate through SocialPostCreate model
       - Return list of SocialPostCreate.model_dump() dicts
       - Skip messages with no text (media-only posts)

    4. **`async def scrape_and_store_channels(channels: list[str] | None = None, limit_per_channel: int = 100) -> int`:**
       - Use OSINT_CHANNELS if channels not provided
       - Ensure client is started: `await client.start()`
       - For each channel: call scrape_channel(), then batch insert to Supabase
       - Use supabase.table("social_posts").upsert(posts, on_conflict="telegram_id,channel").execute()
         Need composite uniqueness -- if Supabase doesn't support multi-column on_conflict easily, use insert with try/except on conflict, or just use insert (duplicates caught by later processing)
         SIMPLER APPROACH: Just use .insert() and accept potential duplicates -- the downstream LLM processor deduplicates by content anyway. Add a UNIQUE constraint on (telegram_id, channel) in the migration if not already there.
       - Return total count of stored posts
       - Handle FloodWaitError > 60s: log warning, sleep, continue with next channel
       - Handle ChannelPrivateError: log warning, skip channel, continue

    5. **`async def close_telegram()`:**
       - Disconnect the client: `await client.disconnect()`

    6. **Error handling:**
       - Wrap each channel scrape in try/except to continue on failures
       - Log channel name, message count for each successful scrape
       - Handle errors.FloodWaitError (>60s), errors.ChannelPrivateError, errors.ChannelInvalidError

    IMPORTANT: Telethon auto-handles FloodWaitError < 60s. Only manually handle waits > 60s. Don't commit .session files (already in .gitignore). First run will require phone number authentication interactively.
  </action>
  <verify>
    - `python -c "from src.fetchers.telegram import scrape_channel, scrape_and_store_channels; print('Telegram scraper imports OK')"`
    - Check FloodWaitError handling: `grep "FloodWaitError" src/fetchers/telegram.py`
    - Check client uses env vars: `grep "TELEGRAM_API" src/fetchers/telegram.py`
  </verify>
  <done>
    Telegram scraper imports cleanly, connects via Telethon with env-var credentials, scrapes configurable OSINT channels, validates through SocialPostCreate, inserts to Supabase social_posts table, handles FloodWaitError and private channel errors gracefully.
  </done>
</task>

</tasks>

<verification>
1. Both fetchers import without errors
2. GDELT fetcher uses domain_exact (not domain) for Chinese state media
3. GDELT fetcher wraps sync gdeltdoc in asyncio.to_thread
4. Telegram scraper handles FloodWaitError and ChannelPrivateError
5. Both fetchers validate data through Pydantic models from src/models/schemas.py
6. Both fetchers use get_supabase() from src/database/client.py
</verification>

<success_criteria>
- GDELT fetcher queries 4 Chinese state media domains via domain_exact and returns validated ArticleCreate dicts
- Telegram scraper connects to 5+ OSINT channels and returns validated SocialPostCreate dicts
- Both fetchers upsert/insert to correct Supabase tables
- Error handling prevents one failed source from crashing the pipeline
- Both modules are callable standalone (for testing) and from FastAPI background tasks (for production)
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-02-SUMMARY.md`
</output>

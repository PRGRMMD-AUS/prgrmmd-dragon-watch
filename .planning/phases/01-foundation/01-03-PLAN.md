---
phase: 01-foundation
plan: 03
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - src/fetchers/ais.py
  - scripts/load_demo_data.py
  - supabase/seed.sql
autonomous: true
user_setup:
  - service: aisstream
    why: "Real-time AIS vessel position data via WebSocket"
    env_vars:
      - name: AISSTREAM_API_KEY
        source: "https://aisstream.io -> Sign up -> Dashboard -> API Key"

must_haves:
  truths:
    - "AIS tracker connects to WebSocket and receives Taiwan Strait vessel positions"
    - "Simulated demo dataset contains 50+ articles, 100+ social posts, and AIS position data"
    - "Demo data loader inserts all simulated data into Supabase tables"
  artifacts:
    - path: "src/fetchers/ais.py"
      provides: "AIS WebSocket client for Taiwan Strait vessel tracking"
      contains: "websockets.connect"
    - path: "scripts/load_demo_data.py"
      provides: "One-time demo data loader script"
      contains: "async def load_demo_data"
    - path: "supabase/seed.sql"
      provides: "SQL seed data for demo scenario"
      contains: "INSERT INTO articles"
  key_links:
    - from: "src/fetchers/ais.py"
      to: "src/database/client.py"
      via: "get_supabase() for position inserts"
      pattern: "get_supabase|supabase.*table.*vessel_positions.*insert"
    - from: "src/fetchers/ais.py"
      to: "src/models/schemas.py"
      via: "VesselPositionCreate for validation"
      pattern: "VesselPositionCreate"
    - from: "scripts/load_demo_data.py"
      to: "src/database/client.py"
      via: "get_supabase() for bulk inserts"
      pattern: "get_supabase"
    - from: "scripts/load_demo_data.py"
      to: "src/models/schemas.py"
      via: "All Create models for validation"
      pattern: "ArticleCreate|SocialPostCreate|VesselPositionCreate"
---

<objective>
Build the AIS WebSocket vessel tracker for Taiwan Strait positions and create the simulated Taiwan Strait demo dataset (50+ articles, 100+ posts, AIS data) with a loader script.

Purpose: DATA-03 (AIS vessel tracking) completes the three live data sources. DATA-04 (simulated demo dataset) is critical for demo reliability -- the demo must work without live API dependencies. Together these ensure both live data capability and offline demo readiness.
Output: AIS WebSocket fetcher module, SQL seed file with demo data, Python loader script.
</objective>

<execution_context>
@/Users/gremmy/.claude/get-shit-done/workflows/execute-plan.md
@/Users/gremmy/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-foundation/01-RESEARCH.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: AIS WebSocket vessel tracker</name>
  <files>src/fetchers/ais.py</files>
  <action>
    Create the AIS WebSocket client that streams vessel positions from the Taiwan Strait and writes to Supabase.

    Create `src/fetchers/ais.py` with:

    1. **Constants:**
       - AIS_WS_URL = "wss://stream.aisstream.io/v0/stream"
       - TAIWAN_STRAIT_BBOX = [[23.0, 118.0], [26.0, 122.0]]  # [[lat_min, lon_min], [lat_max, lon_max]]
       - Add comment explaining bounding box: "Taiwan Strait approximately 23-26N, 118-122E"

    2. **`async def connect_ais_stream(on_position: Callable | None = None, batch_size: int = 50)`:**
       - Use `async for websocket in websockets.connect(AIS_WS_URL):` pattern for auto-reconnect
       - IMMEDIATELY after connect (within 3 seconds -- AISstream.io requirement):
         Send subscription message with:
         - APIKey from AISSTREAM_API_KEY env var
         - BoundingBoxes: [TAIWAN_STRAIT_BBOX]
         - FilterMessageTypes: ["PositionReport"] (only position updates, not ship static data)
       - Receive messages in async for loop
       - Parse each message JSON:
         - Extract from message["Message"]["PositionReport"]: Latitude, Longitude, Sog (speed over ground), Cog (course over ground)
         - Extract from message["MetaData"]: MMSI, ShipName, time_utc
       - Validate through VesselPositionCreate model
       - Batch positions: accumulate in list, flush to Supabase every batch_size positions
       - On flush: get_supabase() -> supabase.table("vessel_positions").insert(batch).execute()
       - If on_position callback provided, call it with each position dict (for real-time processing)

    3. **`async def flush_positions(positions: list[dict]) -> int`:**
       - Bulk insert to Supabase vessel_positions table
       - Return count inserted
       - Handle and log Supabase errors

    4. **Reconnection handling:**
       - websockets.connect() auto-reconnects on ConnectionClosed (outer async for loop)
       - On reconnect, log event and re-subscribe
       - Add asyncio.sleep(5) before reconnect attempt to avoid hammering

    5. **`async def stop_ais_stream()`:**
       - Set a module-level `_running = False` flag that the stream loop checks
       - Allows graceful shutdown

    6. **Error handling:**
       - websockets.ConnectionClosed: auto-reconnect via outer loop
       - json.JSONDecodeError: log bad message, continue
       - KeyError on message parsing: log malformed message, continue (not all messages have same structure)
       - If AISSTREAM_API_KEY not set, raise ValueError immediately

    IMPORTANT: Must subscribe within 3 seconds of connect or connection is dropped. Use narrow bounding box (Taiwan Strait only) to prevent message overflow. Filter to PositionReport only.
  </action>
  <verify>
    - `python -c "from src.fetchers.ais import connect_ais_stream, flush_positions; print('AIS tracker imports OK')"`
    - Check bounding box: `grep "23.0.*118.0.*26.0.*122.0" src/fetchers/ais.py`
    - Check subscription within 3 seconds pattern: `grep "PositionReport" src/fetchers/ais.py`
    - Check auto-reconnect pattern: `grep "async for websocket" src/fetchers/ais.py`
  </verify>
  <done>
    AIS tracker imports cleanly, connects to AISstream.io WebSocket, subscribes to Taiwan Strait bounding box within 3 seconds, filters PositionReport messages, validates through VesselPositionCreate, batch inserts to Supabase, auto-reconnects on disconnect.
  </done>
</task>

<task type="auto">
  <name>Task 2: Simulated Taiwan Strait demo dataset and loader</name>
  <files>
    scripts/load_demo_data.py
    supabase/seed.sql
  </files>
  <action>
    Create the simulated Taiwan Strait escalation demo dataset and a Python script to load it into Supabase.

    The demo scenario simulates a 72-hour Taiwan Strait escalation: GREEN (normal) -> AMBER (heightened) -> RED (imminent). This is the CORE DEMO that must work without any live API dependencies.

    1. **Create `supabase/seed.sql`** with realistic simulated data for the Taiwan Strait scenario:

       **Articles (60+ rows):** Chinese state media articles showing narrative escalation pattern.
       - Hours 0-24 (GREEN): Normal diplomatic coverage. 15-20 articles across xinhuanet.com, globaltimes.cn, cctv.com, people.com.cn. Typical topics: trade, diplomacy, domestic policy. Tone scores neutral (0 to -2).
       - Hours 24-48 (AMBER): Coordinated sovereignty messaging. 20-25 articles. Multiple outlets simultaneously publish "reunification", "sovereignty", "one-China principle" articles. Tone scores increasingly negative (-3 to -6). Similar phrasing across outlets (coordination signal).
       - Hours 48-72 (RED): Military readiness messaging. 20+ articles. "Combat readiness", "military exercises", "defense of territorial integrity". Highest coordination. Tone scores strongly negative (-5 to -10).
       - Each article: realistic URL (e.g., https://www.globaltimes.cn/page/202602/NNNNNN.shtml), realistic title, correct domain, published_at spanning 72 hours from a base timestamp, tone_score, language='English', source_country='China'.
       - Use a base timestamp of '2026-02-05 00:00:00+00' for the scenario start.

       **Social Posts (120+ rows):** Civilian movement reports from simulated Telegram channels.
       - Hours 0-24 (GREEN): Normal posts. 30-40 posts. General OSINT chatter, no military movement signals.
       - Hours 24-48 (AMBER): Movement signals emerge. 40-50 posts. Reports of "military convoy spotted on highway near Xiamen", "unusual naval activity in Fujian ports", "civilian flights rerouted near coast". Mix of channels.
       - Hours 48-72 (RED): Dense movement reports. 50+ posts. "Multiple warships departing port", "convoy of military vehicles heading south", "restricted airspace declared over strait", "fishing boats ordered to return to port". High urgency.
       - Each post: telegram_id (sequential 1000+), channel (from list: @osinttechnical, @IntelDoge, @militarymap, @ChinaOSINT, @TaiwanWatch), realistic text, timestamp spanning 72 hours, views (100-50000).

       **Vessel Positions (200+ rows):** AIS data showing vessel movement patterns.
       - Hours 0-24 (GREEN): Normal shipping traffic. Container ships, tankers, fishing vessels in Taiwan Strait. Regular routes, normal speeds (10-20 knots). 60-80 positions.
       - Hours 24-48 (AMBER): Pattern changes. Some fishing vessels leaving strait area, military-associated MMSI numbers appearing, unusual vessel clustering near Fujian coast. 70-80 positions.
       - Hours 48-72 (RED): Clear military pattern. Civilian vessels dispersing, naval vessel positions (MMSI 4xxxxx series = Chinese naval), concentrated near staging areas. 70-80 positions.
       - Each position: mmsi (civilian: 412xxxxxx Chinese merchant, naval: 4120xxxxx), ship_name, lat (23-26), lon (118-122), speed, course, timestamp spanning 72 hours.
       - Use realistic MMSI numbers: 412000001-412000050 for merchant, 412100001-412100020 for naval.

       The SQL should be INSERT INTO statements that can be run directly. Use the ON CONFLICT DO NOTHING clause on articles (url) to be re-runnable.

    2. **Create `scripts/load_demo_data.py`:**
       - Standalone async Python script (not part of FastAPI app)
       - Reads demo data from Python dicts (not from seed.sql -- more flexible for programmatic generation)
       - Generates the same scenario data programmatically using helper functions:
         - `generate_demo_articles(base_time: datetime, count: int = 60) -> list[dict]`
         - `generate_demo_posts(base_time: datetime, count: int = 120) -> list[dict]`
         - `generate_demo_positions(base_time: datetime, count: int = 200) -> list[dict]`
       - Each generator creates data following the GREEN/AMBER/RED escalation timeline
       - Validates all data through Pydantic models (ArticleCreate, SocialPostCreate, VesselPositionCreate)
       - Inserts into Supabase via get_supabase()
       - Use upsert for articles (on url), plain insert for posts and positions
       - Print summary: "Loaded X articles, Y posts, Z positions"
       - Add `if __name__ == "__main__": asyncio.run(main())` for direct execution

    The seed.sql is a backup/reference. The Python loader is the primary loading mechanism (more flexible, validates through models).

    IMPORTANT: The demo data quality directly determines demo credibility. Article titles and post text must sound realistic, not placeholder. Use real Chinese state media headline patterns and real OSINT channel language patterns.
  </action>
  <verify>
    - `python -c "from scripts.load_demo_data import generate_demo_articles, generate_demo_posts, generate_demo_positions; print('Demo loader imports OK')"`
      OR if scripts is not a package: `python -c "import importlib.util; spec = importlib.util.spec_from_file_location('loader', 'scripts/load_demo_data.py'); mod = importlib.util.module_from_spec(spec); spec.loader.exec_module(mod); print('Demo loader OK')"`
    - Check article count: verify generate_demo_articles returns 50+ items
    - Check post count: verify generate_demo_posts returns 100+ items
    - Check position count: verify generate_demo_positions returns 200+ items
    - Check seed.sql contains INSERT INTO for articles, social_posts, vessel_positions
    - `wc -l supabase/seed.sql` should be 300+ lines (substantial demo data)
  </verify>
  <done>
    Demo dataset contains 60+ articles (escalating narrative), 120+ social posts (movement signals), 200+ vessel positions (pattern changes) across a 72-hour GREEN-AMBER-RED scenario. Python loader validates all data through Pydantic models and inserts to Supabase. SQL seed file provides backup insert mechanism.
  </done>
</task>

</tasks>

<verification>
1. AIS tracker imports and uses correct WebSocket URL and Taiwan Strait bounding box
2. AIS tracker subscribes with PositionReport filter within connection handler
3. Demo data generator produces 50+ articles, 100+ posts, 200+ positions
4. Demo data follows realistic 72-hour escalation pattern (GREEN -> AMBER -> RED)
5. Both modules use Pydantic models from src/models/schemas.py for validation
6. Both modules use get_supabase() from src/database/client.py for database access
</verification>

<success_criteria>
- AIS WebSocket tracker connects, subscribes to Taiwan Strait, parses PositionReport messages, batch inserts to Supabase
- Demo dataset has 60+ articles with escalating narrative coordination pattern
- Demo dataset has 120+ posts with escalating civilian movement signals
- Demo dataset has 200+ vessel positions with pattern changes over 72 hours
- Python loader script is runnable standalone: `python scripts/load_demo_data.py`
- SQL seed file is runnable as backup: can be applied in Supabase SQL editor
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-03-SUMMARY.md`
</output>
